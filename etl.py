from pyspark.sql import SparkSession
from pyspark.sql.functions import col ,isnan,expr,round , when, max
from datetime import datetime
import psycopg2
import boto3

s3_client = boto3.client('s3')
bucket_name = 'savecsvintoprasads3bucket'
folder_name = 'inputfile'
file_name = 'dataFile.csv' 
file_path = '/tmp/' + file_name
s3_key = folder_name + '/' + file_name
s3_client.download_file(bucket_name, s3_key, file_path)

spark = SparkSession.builder.appName("FileProcessing").getOrCreate()  
df = spark.read.format("csv").option("header","true").option("inferSchema","true").load('s3a://savecsvintoprasads3bucket.s3.amazonaws.com/inputfile/dataFile.csv')
df.show()

df.describe().show()

df = df.filter( (col('Product').isNotNull()) & (col('UnitsSold').isNotNull()) & (col('Revenue').isNotNull()) & (col('Product') != 'NULL') & (col('Date') != 'NULL') & (col('Revenue') != 'NULL'))

df = df.withColumn('unit_price', round(expr('revenue/unitsSold'),2))

df = df.withColumn('cost_price', when(col('product') == 'Product A', 11)
                                .when(col('product') == 'Product B', 11.2)
                                .otherwise(10))

df = df.withColumn('profit_percent', round(((col('unit_price') - col('cost_price'))*100 / col('unit_price')), 2).cast('double'))

df.createOrReplaceTempView("sales_data")

avg_product_price = spark.sql("""
    SELECT Region, Product, AVG(unit_price) AS avg_product_price
    FROM sales_data
    GROUP BY Region, Product
""")

max_unit_price = spark.sql("""
    SELECT Region, MAX(unit_price) AS max_unit_price
    FROM sales_data
    GROUP BY Region
""")

max_profit_per_product = spark.sql("""
    SELECT Region, Product, MAX(profit_percent) AS max_profit
    FROM sales_data
    GROUP BY Region, Product
""")

max_profit_per_product.show()


db_host = 'salesdb.cluster-cwtacdgkluou.us-east-1.rds.amazonaws.com'
db_name = 'abc_sales'
db_user = 'postgres'
db_password = 'postgres'
db_port = '5432'
    
conn = psycopg2.connect(
    host=db_host,
    dbname=db_name,
    user=db_user,
    password=db_password,
    port=db_port
)
cursor = conn.cursor()
cursor.execute('''
CREATE TABLE IF NOT EXISTS public.abc_sales
(
id bigint NOT NULL GENERATED BY DEFAULT AS IDENTITY ( INCREMENT 1 START 1 MINVALUE 1 MAXVALUE 99999999999 CACHE 1 ),
region character varying(20) COLLATE pg_catalog."default",
product character varying(20) COLLATE pg_catalog."default",
date timestamp without time zone,
unitsold bigint,
revenue character varying(50) COLLATE pg_catalog."default",
unit_price double precision,
cost_price double precision,
profit_percent double precision,
CONSTRAINT abc_sales_pkey PRIMARY KEY (id)
    )''')
conn.commit()


df_values = df.collect()
    
for row in df_values:
    cursor.execute('''INSERT INTO public.abc_sales (region, product, date, unitsold, revenue, unit_price, cost_price, profit_percent) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)''',     
    (row['Region'],
    row['Product'],
    row['Date'],
    row['UnitsSold'],
    row['Revenue'],
    row['unit_price'],
    row['cost_price'],
    row['profit_percent']))
    conn.commit()
conn.close()

